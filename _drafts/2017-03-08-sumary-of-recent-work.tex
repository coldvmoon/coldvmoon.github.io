% Created 2017-03-10 周五 13:47
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.1.1 (Org mode 9.0.3)}, 
 pdflang={English}}
\begin{document}

\tableofcontents

\section{Sentry Authentication}
\label{sec:orge633d17}
\subsection{LDAP Installation}
\label{sec:org361e2f3}
\subsection{Hive Configuration}
\label{sec:org97b1ffb}
\section{Hive Debug}
\label{sec:org6eacff1}
\section{Hive Hook}
\label{sec:org686d572}
\subsection{Hooks lifecycle}
\label{sec:orgdb0e80e}
There are several types of hooks depending on at which stage you want to inject your custom code:
\begin{itemize}
\item Driver run hooks (Pre/Post)
\item Semantic analyizer hooks (Pre/Post)
\item Execution hooks (Pre/Failure/Post)
\item Client statistics publisher
\end{itemize}
If you run a script the processing flow looks like as follows:
\begin{enumerate}
\item \textbf{Driver.run()} takes the command
\item \textbf{HiveDriverRunHook.preDriverRun()} \texttt{HiveConf.ConfVars.HIVE\_DRIVER\_RUN\_HOOKS}
\item \textbf{Driver.compile()} starts processing the command: creates the abstract syntax tree
\item \textbf{AbstractSemanticAnalyzerHook.preAnalyze()} \texttt{AbstractSemanticAnalyzerHook.preAnalyze()}
\item Semantic analysis
\item \textbf{AbstractSemanticAnalyzerHook.postAnalyze()} \texttt{HiveConf.ConfVars.SEMANTIC\_ANALYZER\_HOOK}
\item Create and validate the query plan (physical plan)
\item \textbf{Driver.execute()} : ready to run the jobs
\item \textbf{ExecuteWithHookContext.run()}  \texttt{HiveConf.ConfVars.PREEXECHOOKS}
\item \textbf{ExecDriver.execute()} runs all the jobs
\item For each job at every \texttt{HiveConf.ConfVars.HIVECOUNTERSPULLINTERVAL} interval:*ClientStatsPublisher.run()* is called to publish statistics \texttt{HiveConf.ConfVars.CLIENTSTATSPUBLISHERS} If a task fails: \textbf{ExecuteWithHookContext.run()} \texttt{HiveConf.ConfVars.ONFAILUREHOOKS}
\item Finish all the tasks
\item \textbf{ExecuteWithHookContext.run()} \texttt{HiveConf.ConfVars.POSTEXECHOOKS}
\item Before returning the result \textbf{HiveDriverRunHook.postDriverRun()} \texttt{HiveConf.ConfVars.HIVE\_DRIVER\_RUN\_HOOKS}
\item Return the result.
\end{enumerate}

\subsection{Usage in DDW}
\label{sec:orga758aca}
\subsubsection{ExecuteWithHookContext}
\label{sec:org9a5dfb2}
\begin{enumerate}
\item 设置HiveConf的mapreduce.job.queuename值，方便CDH中的资源池管理。
\item HiveHistory文件追踪，监控Hive SQL的进度
\end{enumerate}
\begin{verbatim}
 /**
 * 1 ) 用于将用户提交到对应的pool中,这样便于统一对某个用户执行资源(mem/cpu)使用控制,防止其占用过多资源卡住其他job
 * 2 ) 创建任务
 * Created by xkwu on 2017/1/17.
 */
public class CustomExecuteWithHookContext implements ExecuteWithHookContext {
    private static final Log LOG = LogFactory.getLog(CustomExecuteWithHookContext.class);
    private static TailerTracker tailerTracker = new TailerTracker();

    static {
        tailerTracker.start();
    }
    @Override
    public void run(HookContext hookContext) throws Exception {
        try {
        UserGroupInformation ugi = hookContext.getUgi();
        hookContext.getConf().set("mapreduce.job.queuename", ugi.getUserName());//1)修改queuename

            QueryPlan queryPlan = hookContext.getQueryPlan();
            int jobs = Utilities.getMRTasks(queryPlan.getRootTasks()).size();
            if (jobs > 0) {
                String histFileName = SessionState.get().getHiveHistory().getHistFileName();
                tailerTracker.track(hookContext.getUserName(), histFileName);//2)创建监控HiveHistory的listener
            }
        } catch (Exception e) {
            LOG.error(e.getMessage(), e);
        }
    }
}
\end{verbatim}
\subsubsection{AbstractSemanticAnalyzerHook}
\label{sec:org601522a}
\begin{enumerate}
\item 禁止drop databases,create databases等操作
\item 将创建未压缩table的语句以邮件的形式发送给管理员
\end{enumerate}
\begin{verbatim}
/**
 *
 * 主要用于过滤出创建表,却不指定采用压缩格式.
 * 便于我们及时跟进指导业务方修正
 *
 * Created by xkwu on 2017/1/18.
 */
public class CustomSemanticAnalyzerHook extends AbstractSemanticAnalyzerHook {
    private static final Log LOG = LogFactory.getLog(CustomSemanticAnalyzerHook.class);
    private Pattern pattern = Pattern.compile("(STORED\\s+?AS\\s+?)(parquet|rcfile|orcfile|orc)", Pattern.CASE_INSENSITIVE);   // 正则匹配采用了压缩存储格式的表scheme

    @Override
    public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context,
                              ASTNode ast) throws SemanticException {
        switch (ast.getToken().getType()) {
            case HiveParser.TOK_CREATETABLE:
                Matcher m = pattern.matcher(context.getCommand());
                if (!m.find()) {
                    LOG.info(context.getUserName() + "执行了操作: " + context.getCommand());
                    new EmailThread("/hive/notifyAboutCreateTable", "cmd=" + context.getCommand() + "&groupCode=" + context.getUserName()).start();
                }
                break;
            case HiveParser.TOK_CREATEDATABASE:
            case HiveParser.TOK_DROPDATABASE:
                String adminAccount = ConfigUtils.prop.getProperty("ADMIN_ACCOUNT");
                if (StringUtils.isEmpty(context.getUserName()) || !context.getUserName().equals(adminAccount)) {
                    throw new SemanticException("Only the admin accounts of ddw bigdata platform can create/drop database.");
                }
                break;
            default:
                break;
        }
        return ast;
    }
}
\end{verbatim}
\end{document}