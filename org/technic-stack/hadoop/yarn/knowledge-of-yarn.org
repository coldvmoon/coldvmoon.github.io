#+TITLE: Knowledge Of Yarn
* ResourceManager High Availability[fn:1]
** Architecture
[[file:fair-over-architecture.png]]
** RM Failover
ResourceManager HA is realized through an Active/Standby architecture - at any
point of time, one of the RMs is Active, and one or more RMs are in Standby mode
waiting to take over should anything happen to the Active. The trigger to
transition-to-active comes from either the admin (through CLI) or through the
integrated failover-controller when automatic-failover is enabled.
*** Manual transitions and failover
Transition can be done using the "yarn rmadmin" CLI.
*** Automatic failover
The RMs have an option to embed the Zookeeper-based ActiveStandbyElector to
decide which RM should be the Active.
*** Client, ApplicationMaster and NodeManager on RM failover
 Clients, ApplicationMasters (AMs) and NodeManagers (NMs) try connecting to the
 RMs in a round-robin fashion until they hit the Active RM. Ifthe Active goes
 down, they resume the round-robin polling until they hit the “new” Active.
** Recovering prevous active-RM’s state
With the ResourceManger Restart enabled, the RM being promoted to an active
state loads the RM internal state and continues to operate from where the
previous active left off as much as possible depending on the RM restart
feature. A new attempt is spawned for each managed application previously
submitted to the RM. Applications can checkpoint periodically to avoid losing
any work. The state-store must be visible from the both of Active/Standby RMs.
Currently, there are two RMStateStore implementations for persistence -
~FileSystemRMStateStore~ and ~ZKRMStateStore.~ The ZKRMStateStore implicitly
allows write access to a single RM at any point in time, and hence is the
recommended store to use in an HA cluster. When using the ZKRMStateStore, there
is no need for a separate fencing mechanism to address a potential split-brain
situation where multiple RMs can potentially assume the Active role. When using
the ZKRMStateStore, it is advisable to NOT set the
“zookeeper.DigestAuthenticationProvider.superDigest” property on the Zookeeper
cluster to ensure that the zookeeper admin does not have access to YARN
application/user credential information.
* Logs[fn:2]
Once the job has completed the NodeManager will keep the log for each container
for ${yarn.nodemanager.log.retain-seconds} which is 10800 seconds by default ( 3
hours ) and delete them once they have expired. But if
${yarn.log-aggregation-enable} is enabled then the NodeManager will immediately
concatenate all of the containers logs into one file and upload them into HDFS
in ${yarn.nodemanager.remote-app-log-dir}/${user.name}/logs/ and delete them
from the local userlogs directory
** Localize Logs
** Aggregation Logs
~JobHistory Server use =AggregatedLogDeletionService= to clean aggregation logs
in hdfs(user =mapred= to access hdfs).~
#+BEGIN_SRC java
  Permission denied: user=mapred, access=EXECUTE, inode="/tmp/logs/ddw_ddc":ddw_ddc:hdfs:drwxrwx---
	  at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:281)
	  at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:262)
	  at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkTraverse(DefaultAuthorizationProvider.java:206)
	  at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:158)
	  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:152)
	  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6621)
	  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6603)
	  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPathAccess(FSNamesystem.java:6528)
	  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListingInt(FSNamesystem.java:5062)
	  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(FSNamesystem.java:5023)
	  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing(NameNodeRpcServer.java:884)
	  at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getListing(AuthorizationProviderProxyClientProtocol.java:336)
	  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getListing(ClientNamenodeProtocolServerSideTranslatorPB.java:620)
	  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
	  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
	  at java.security.AccessController.doPrivileged(Native Method)
	  at javax.security.auth.Subject.doAs(Subject.java:422)
	  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)
#+END_SRC
* Footnotes
[fn:1] [[https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html][ResourceManager High Availability]] 
[fn:2] [[http://stackoverflow.com/questions/39195478/how-to-delete-yarn-logs][How to delete yarn logs]]
